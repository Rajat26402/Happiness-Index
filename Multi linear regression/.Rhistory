print(accuracy)
write.csv(cbind(Y,Yp),"Y_vs_YPredicted.csv")
df=read.csv("2016_withNA.csv")
df=subset(df,select= -c(Country,Region,Happiness.Rank,Year))
df<-na.omit(df)
colnames(df)
str(df)
model <- lm(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data = df)
print(model)
require(ggplot2)
ggplot(df,aes(y=Happiness.Score,x=Trust..Government.Corruption.,color=Economy..GDP.per.Capita.))+geom_point()+stat_smooth(method="lm",se=FALSE)
ggplot(df,aes(y=Happiness.Score,x=Lower.Confidence.Interval,color=Upper.Confidence.Interval))+geom_point()+stat_smooth(method="lm",se=FALSE)
cat("# # # # The Coefficient Values # # # ","\n")
a <- coef(model)[1]
print(a)
for( i in 2:length(coef(model))){
print(coef(model)[i])
}
df=read.csv("2017_withNA.csv")
df=subset(df,select= -c(Country,Happiness.Rank,Year))
df<-na.omit(df)
colnames(df)
X=subset(df,select = -c(Happiness.Score))
Y=subset(df,select = c(Happiness.Score))
Yp=c()
sum=0
for(i in 1:dim(X)[1]){
sum=coef(model)[1]
for( j in 2:length(coef(model))){
sum=sum+coef(model)[j]*X[i,j-1]
}
Yp=c(Yp,sum)
}
Yp=data.frame(Happiness.Score.predict=Yp)
str(Y)
str(Yp)
Y_mean=sum(Y)/nrow(Y)
numerator=sum((Y-Yp)^2)
denominator=sum((Y-Y_mean)^2)
msr=1-numerator/denominator
print(paste("R square: ",msr))
error=(sum(abs(Yp-Y)))/sum(Y)*100
accuracy=100-error
print(paste("Accuracy: ",accuracy,"%"))
write.csv(cbind(Y,Yp),"Y_vs_YPredictedwithNA.csv")
#Preprocessed
df=read.csv("2016_new.csv")
df=subset(df,select= -c(Country,Region,Happiness.Rank,Happiness.Rank.1,Year))
colnames(df)
str(df)
model <- lm(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data = df)
print(model)
require(ggplot2)
ggplot(df,aes(y=Happiness.Score,x=Trust..Government.Corruption.,color=Economy..GDP.per.Capita.))+geom_point()+stat_smooth(method="lm",se=FALSE)
ggplot(df,aes(y=Happiness.Score,x=Lower.Confidence.Interval,color=Upper.Confidence.Interval))+geom_point()+stat_smooth(method="lm",se=FALSE)
cat("# # # # The Coefficient Values # # # ","\n")
a <- coef(model)[1]
print(a)
for( i in 2:length(coef(model))){
print(coef(model)[i])
}
df=read.csv("2017_new.csv")
df=subset(df,select= -c(Country,Happiness.Rank,Happiness.Rank.1,Year))
colnames(df)
X=subset(df,select = -c(Happiness.Score))
Y=subset(df,select = c(Happiness.Score))
Yp=c()
sum=0
for(i in 1:dim(X)[1]){
sum=coef(model)[1]
for( j in 2:length(coef(model))){
sum=sum+coef(model)[j]*X[i,j-1]
}
Yp=c(Yp,sum)
}
Yp=data.frame(Happiness.Score.predict=Yp)
str(Y)
str(Yp)
Y_mean=sum(Y)/nrow(Y)
numerator=sum((Y-Yp)^2)
denominator=sum((Y-Y_mean)^2)
r2=1-numerator/denominator
print(paste("R square: ",r2))
error=(sum(abs(Yp-Y)))/sum(Y)*100
accuracy=100-error
print(accuracy)
write.csv(cbind(Y,Yp),"Y_vs_YPredicted.csv")
# Loading Data
train<-read.csv("2016_new.csv")
test<-read.csv("2017_new.csv")
# Viewing data
head(train,5)
head(test,5)
# Dividing columns into y and x of training data.
y<-train$Happiness.Score
x<-data.matrix(train[,c(6,7,8,9,10,11,12,13,14)])
# Loading Library
library(glmnet)
# Modeling using training.
cv_model<-cv.glmnet(x,y,alpha=1)
best_lambda<-cv_model$lambda.min
best_lambda
# Ploting the Model
plot(cv_model)
#Finding Model
best_model<-glmnet(x,y,alpha = 1,lambda = best_lambda)
coef(best_model)
# Predicting data for testing data
Y<-test$Happiness.Score
X<-data.matrix(test[,c(5,6,7,8,9,10,11,12,13)])
y_predicted<-predict(best_model,s=best_lambda,newx = X)
# Finding Accuracy
sst<-sum((Y-mean(Y))^2)
sse<-sum((y_predicted-Y)^2)
rsq<-1-sse/sst
acc = rsq * 100
acc
# Finding R2
sst<-sum((Y-mean(Y))^2)
sse<-sum((y_predicted-Y)^2)
rsq<-1-sse/sst
rsq
#ridge model old - after omitting NA values
#install.packages("cowplot")
#install.packages("caret")
#install.packages("xgboost")
#install.packages("lava")
# load packages
library(data.table) # used for reading and manipulation of data
library(dplyr)	 # used for data manipulation and joining
library(glmnet)	 # used for regression
library(ggplot2) # used for plotting
library(caret)	 # used for modeling
library(xgboost) # used for building XGBoost model
library(e1071)	 # used for skewness
library(cowplot) # used for combining multiple plots
# Loading datasets
train = read.csv("2016_withNA.csv")
test = read.csv("2017_withNA.csv")
#cleaning dataset - omiting NA values
train<-na.omit(train)
nrow(train)
test<-na.omit(test)
nrow(test)
# Model Building :Lasso Regression
# set.seed(123)
# control = trainControl(method ="cv", number = 5)
# Grid_reg = expand.grid(alpha = 1, lambda = seq(0.001,
#                                                   0.1, by = 0.0002))
# Model Building : Ridge Regression
set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_reg = expand.grid(alpha = 0, lambda = seq(0.001, 0.1,
by = 0.0002))
x = train[ , -c(1,2,3,13)]
y = train$Happiness.Score
# Training Ridge Regression model
Ridge_model = train(x,
y,
method = "glmnet",
trControl = control,
tuneGrid = Grid_reg
)
Ridge_model
# mean validation score
mean=mean(Ridge_model$resample$RMSE)
# accuracy
acc = 100-(mean*100)
print(paste("Accuracy=",acc,"%"))
# Plot
plot(Ridge_model, main="Ridge Regression")
# Loading Data
train<-read.csv("2016_withNA.csv")
test<-read.csv("2017_withNA.csv")
# Omitting NA values
train<-na.omit(train)
nrow(train)
test<-na.omit(test)
# Viewing data
head(train,5)
head(test,5)
# Dividing columns into y and x of training data.
y<-train$Happiness.Score
x<-data.matrix(train[,c(5,6,7,8,9,10,11,12,13)])
# Loading Library
library(glmnet)
# Modeling using training.
cv_model<-cv.glmnet(x,y,alpha=1)
best_lambda<-cv_model$lambda.min
best_lambda
# Ploting the Model
plot(cv_model)
#Finding Model
best_model<-glmnet(x,y,alpha = 1,lambda = best_lambda)
coef(best_model)
# Predicting data for testing data
Y<-test$Happiness.Score
X<-data.matrix(test[,c(4,5,6,7,8,9,10,11,12)])
y_predicted<-predict(best_model,s=best_lambda,newx = X)
# Finding Accuracy
sst<-sum((Y-mean(Y))^2)
sse<-sum((y_predicted-Y)^2)
rsq<-1-sse/sst
acc = rsq * 100
acc
acc=100-(sum(y_predicted-Y)/length(y_predicted))*100
acc
# Loading Data
train<-read.csv("2016_new.csv")
test<-read.csv("2017_new.csv")
# Viewing data
head(train,5)
head(test,5)
# Dividing columns into y and x of training data.
y<-train$Happiness.Score
x<-data.matrix(train[,c(6,7,8,9,10,11,12,13,14)])
# Loading Library
library(glmnet)
# Modeling using training.
cv_model<-cv.glmnet(x,y,alpha=1)
best_lambda<-cv_model$lambda.min
best_lambda
# Ploting the Model
plot(cv_model)
#Finding Model
best_model<-glmnet(x,y,alpha = 1,lambda = best_lambda)
coef(best_model)
# Predicting data for testing data
Y<-test$Happiness.Score
X<-data.matrix(test[,c(5,6,7,8,9,10,11,12,13)])
y_predicted<-predict(best_model,s=best_lambda,newx = X)
# Finding R2
sst<-sum((Y-mean(Y))^2)
sse<-sum((y_predicted-Y)^2)
rsq<-1-sse/sst
rsq
# Finding Accuracy
acc=100-(sum(y_predicted-Y)/length(y_predicted))*100
acc
install.packages("pls")
library(pls)
data1 <- read.csv("2016_withNA.csv", header=TRUE, stringsAsFactors=FALSE)
data1<-na.omit(data1)
head(data1)
set.seed(1)    #fit PCR model
model <- plsr(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data=data1, scale=TRUE, validation="CV")
#view summary of model fitting
summary(model)
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")
#define training and testing sets
data2 <- read.csv("2017_withNA.csv", header=TRUE, stringsAsFactors=FALSE)
data2<-na.omit(data2)
train <- data2[ , c("Upper.Confidence.Interval","Lower.Confidence.Interval","Economy..GDP.per.Capita." ,"Family","Health..Life.Expectancy." , "Freedom" ,"Trust..Government.Corruption.","Generosity","Dystopia.Residual")]
y_test <- data2[  , c("Happiness.Score")]
#use model to make predictions on a test set
pcr_pred <- predict(model, train, ncomp=2)
acc=100-(sum(pcr_pred-y_test)/length(pcr_pred))*100
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((train-mean(train))^2)
sse<-sum((pcr_pred-train)^2)
rsq<-1-sse/sst
rsq
train <- data2[ , c("Upper.Confidence.Interval","Lower.Confidence.Interval","Economy..GDP.per.Capita." ,"Family","Health..Life.Expectancy." , "Freedom" ,"Trust..Government.Corruption.","Generosity","Dystopia.Residual")]
y_test <- data2[  , c("Happiness.Score")]
#use model to make predictions on a test set
pcr_pred <- predict(model, train, ncomp=2)
acc=100-(sum(pcr_pred-y_test)/length(pcr_pred))*100
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((train-mean(train))^2)
sse<-sum((pcr_pred-train)^2)
rsq<-1-sse/sst
rsq
sst<-sum((y_test-mean(y_test))^2)
sse<-sum((pcr_pred-y_test)^2)
rsq<-1-sse/sst
rsq
install.packages("pls")
library(pls)
data1 <- read.csv("2016_new.csv", header=TRUE, stringsAsFactors=FALSE)
head(data1)
set.seed(1)
#fit PCR model
model <- plsr(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data=data1, scale=TRUE, validation="CV")
#view summary of model fitting
summary(model)
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")
#define training and testing sets
data2 <- read.csv("2017_new.csv", header=TRUE, stringsAsFactors=FALSE)
train <- data2[ , c("Upper.Confidence.Interval","Lower.Confidence.Interval","Economy..GDP.per.Capita." ,"Family","Health..Life.Expectancy." , "Freedom" ,"Trust..Government.Corruption.","Generosity","Dystopia.Residual")]
y_test <- data2[  , c("Happiness.Score")]
#use model to make predictions on a test set
pcr_pred <- predict(model, train, ncomp=2)
acc=100-(sum(pcr_pred-y_test)/length(pcr_pred))*100
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((y_test-mean(y_test))^2)
sse<-sum((pcr_pred-y_test)^2)
rsq<-1-sse/sst
rsq
install.packages("pls")
#install.packages("pls")
library(pls)
data1 <- read.csv("2016_new.csv", header=TRUE, stringsAsFactors=FALSE)
head(data1)
set.seed(1)
#fit PCR model
model <- plsr(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data=data1, scale=TRUE, validation="CV")
#view summary of model fitting
summary(model)
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")
#define training and testing sets
data2 <- read.csv("2017_new.csv", header=TRUE, stringsAsFactors=FALSE)
train <- data2[ , c("Upper.Confidence.Interval","Lower.Confidence.Interval","Economy..GDP.per.Capita." ,"Family","Health..Life.Expectancy." , "Freedom" ,"Trust..Government.Corruption.","Generosity","Dystopia.Residual")]
y_test <- data2[  , c("Happiness.Score")]
#use model to make predictions on a test set
pcr_pred <- predict(model, train, ncomp=2)
acc=100-(sum(pcr_pred-y_test)/length(pcr_pred))*100
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((y_test-mean(y_test))^2)
sse<-sum((pcr_pred-y_test)^2)
rsq<-1-sse/sst
rsq
#install.packages("pls")
library(pls)
data1 <- read.csv("2016_new.csv", header=TRUE, stringsAsFactors=FALSE)
head(data1)
set.seed(1)
#fit PCR model
model <- plsr(Happiness.Score~Lower.Confidence.Interval+Upper.Confidence.Interval+Economy..GDP.per.Capita.+Family+Health..Life.Expectancy.+Freedom+Trust..Government.Corruption.+Generosity+Dystopia.Residual, data=data1, scale=TRUE, validation="CV")
#view summary of model fitting
summary(model)
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")
#define training and testing sets
data2 <- read.csv("2017_new.csv", header=TRUE, stringsAsFactors=FALSE)
y_train <- data2[ , c("Upper.Confidence.Interval","Lower.Confidence.Interval","Economy..GDP.per.Capita." ,"Family","Health..Life.Expectancy." , "Freedom" ,"Trust..Government.Corruption.","Generosity","Dystopia.Residual")]
y_test <- data2[  , c("Happiness.Score")]
#use model to make predictions on a test set
pcr_pred <- predict(model, y_train, ncomp=2)
acc=100-(sum(pcr_pred-y_test)/length(pcr_pred))*100
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((y_train-mean(y_train))^2)
sse<-sum((pcr_pred-y_train)^2)
rsq<-1-sse/sst
rsq
#ridge model new - after imputing NA values with median values
#install.packages("cowplot")
#install.packages("caret")
#install.packages("xgboost")
#install.packages("lava")
# load packages
library(data.table) # used for reading and manipulation of data
library(dplyr)	 # used for data manipulation and joining
library(glmnet)	 # used for regression
library(ggplot2) # used for ploting
library(caret)	 # used for modeling
library(xgboost) # used for building XGBoost model
library(e1071)	 # used for skewness
library(cowplot) # used for combining multiple plots
# Loading datasets
train = read.csv("2016_new.csv")
test = read.csv("2017_new.csv")
#cleaning dataset - omiting NA values
# train<-na.omit(train)
# nrow(train)
# test<-na.omit(test)
# nrow(test)
# Model Building :Lasso Regression
# set.seed(123)
# control = trainControl(method ="cv", number = 5)
# Grid_reg = expand.grid(alpha = 1, lambda = seq(0.001,
#                                                0.1, by = 0.0002))
# Model Building : Ridge Regression
set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_reg = expand.grid(alpha = 0, lambda = seq(0.001, 0.1,
by = 0.0002))
x = train[ , -c(1,2,3,13)]
y = train$Happiness.Score
# Training Ridge Regression model
Ridge_model = train(x,
y,
method = "glmnet",
trControl = control,
tuneGrid = Grid_reg
)
Ridge_model
# mean validation score
mean=mean(Ridge_model$resample$RMSE)
# accuracy
acc = 100-(mean*100)
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((y_train-mean(y_train))^2)
sse<-sum((pcr_pred-y_train)^2)
rsq<-1-sse/sst
rsq
# Plot
plot(Ridge_model, main="Ridge Regression")
#ridge model new - after imputing NA values with median values
#install.packages("cowplot")
#install.packages("caret")
#install.packages("xgboost")
#install.packages("lava")
# load packages
library(data.table) # used for reading and manipulation of data
library(dplyr)	 # used for data manipulation and joining
library(glmnet)	 # used for regression
library(ggplot2) # used for ploting
library(caret)	 # used for modeling
library(xgboost) # used for building XGBoost model
library(e1071)	 # used for skewness
library(cowplot) # used for combining multiple plots
# Loading datasets
train = read.csv("2016_new.csv")
test = read.csv("2017_new.csv")
#cleaning dataset - omiting NA values
# train<-na.omit(train)
# nrow(train)
# test<-na.omit(test)
# nrow(test)
# Model Building :Lasso Regression
# set.seed(123)
# control = trainControl(method ="cv", number = 5)
# Grid_reg = expand.grid(alpha = 1, lambda = seq(0.001,
#                                                0.1, by = 0.0002))
# Model Building : Ridge Regression
set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_reg = expand.grid(alpha = 0, lambda = seq(0.001, 0.1,
by = 0.0002))
x = train[ , -c(1,2,3,13)]
y = train$Happiness.Score
# Training Ridge Regression model
Ridge_model = train(x,
y,
method = "glmnet",
trControl = control,
tuneGrid = Grid_reg
)
Ridge_model
# mean validation score
mean=mean(Ridge_model$resample$RMSE)
# accuracy
acc = 100-(mean*100)
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((train-mean(train))^2)
#ridge model new - after imputing NA values with median values
#install.packages("cowplot")
#install.packages("caret")
#install.packages("xgboost")
#install.packages("lava")
# load packages
library(data.table) # used for reading and manipulation of data
library(dplyr)	 # used for data manipulation and joining
library(glmnet)	 # used for regression
library(ggplot2) # used for ploting
library(caret)	 # used for modeling
library(xgboost) # used for building XGBoost model
library(e1071)	 # used for skewness
library(cowplot) # used for combining multiple plots
# Loading datasets
train = read.csv("2016_new.csv")
test = read.csv("2017_new.csv")
#cleaning dataset - omiting NA values
# train<-na.omit(train)
# nrow(train)
# test<-na.omit(test)
# nrow(test)
# Model Building :Lasso Regression
# set.seed(123)
# control = trainControl(method ="cv", number = 5)
# Grid_reg = expand.grid(alpha = 1, lambda = seq(0.001,
#                                                0.1, by = 0.0002))
# Model Building : Ridge Regression
set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_reg = expand.grid(alpha = 0, lambda = seq(0.001, 0.1,
by = 0.0002))
x = train[ , -c(1,2,3,13)]
y = train$Happiness.Score
# Training Ridge Regression model
Ridge_model = train(x,
y,
method = "glmnet",
trControl = control,
tuneGrid = Grid_reg
)
Ridge_model
# mean validation score
mean=mean(Ridge_model$resample$RMSE)
# accuracy
acc = 100-(mean*100)
print(paste("Accuracy=",acc,"%"))
#R2
sst<-sum((test-mean(test))^2)
